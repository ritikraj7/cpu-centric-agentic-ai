#!/usr/bin/env python3
"""
Parallel Retrieval Batch Benchmark - Load index ONCE, run batch sizes SEQUENTIALLY with parallel retrieval.
 
This script:
1. Loads FAISS index and docstore ONCE at the start
2. Runs batch sizes sequentially (1, 2, 4, 8, ...)
3. Each batch uses multiprocessing for PARALLEL retrieval (like generation)
4. Generation still uses threads = batch_size for parallel generation
"""
 
import argparse
import json
import sys
import time
from pathlib import Path
import os
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor
 
# Import from the RAG module
from retrieval import LargeScaleRAGRetriever
 
 
def run_single_batch_benchmark(
    retriever: LargeScaleRAGRetriever,
    batch_size: int,
    queries: List[str],
    top_k: int,
    max_chars_per_doc: int,
    rag_workers: int,
) -> Dict:
    """Run benchmark for a single batch size using pre-loaded retriever."""
 
    print(f"\n{'=' * 80}")
    print(f"Batch Size: {batch_size} (rag_workers={rag_workers})")
    print("=" * 80)
 
    # Take only batch_size queries
    batch_queries = queries[:batch_size]
 
    overall_start = time.perf_counter()
 
    # Phase 1: Parallel retrieval using threading (like generation)
    print(f"[1/2] Parallel retrieval for {len(batch_queries)} queries with {rag_workers} workers...")
    retrieval_start = time.perf_counter()
 
    def retrieve_single_query(query_idx: int) -> tuple:
        """Retrieve documents for a single query."""
        query = batch_queries[query_idx]
        docs, stats = retriever.retrieve_batch(
            queries=[query],
            top_k=top_k
        )
        return (query_idx, docs[0], stats)
 
    # Execute retrieval in parallel using threading
    all_docs = [None] * len(batch_queries)
    all_retrieval_stats = []
 
    if rag_workers > 1:
        with ThreadPoolExecutor(max_workers=batch_size) as executor:
            futures = [executor.submit(retrieve_single_query, i) for i in range(len(batch_queries))]
            for future in futures:
                query_idx, docs, stats = future.result()
                all_docs[query_idx] = docs
                all_retrieval_stats.append(stats)
    else:
        for i in range(len(batch_queries)):
            query_idx, docs, stats = retrieve_single_query(i)
            all_docs[query_idx] = docs
            all_retrieval_stats.append(stats)
 
    batch_retrieval_time = time.perf_counter() - retrieval_start
    print(f"      âœ“ Retrieval: {batch_retrieval_time*1000:.1f}ms")
 
    # Phase 2: Parallel generation (vLLM handles parallelization automatically)
    print(f"[2/2] Batch generation (vLLM auto-parallelizes {len(batch_queries)} requests)...")
 
    # Pre-process: Convert documents to Haystack format and build prompts (not timed)
    print(f"      Preparing prompts...")
    prep_start = time.perf_counter()
    prepared_prompts = []
    for i, query in enumerate(batch_queries):
        docs = all_docs[i]
 
        # Convert to Haystack format
        from haystack.dataclasses import Document as HaystackDocument
        haystack_docs = []
        for doc in docs:
            content = doc.get("content", "")[:max_chars_per_doc]
            meta = doc.get("meta", {})
            haystack_docs.append(
                HaystackDocument(
                    content=content,
                    meta={
                        "source_file": meta.get("source_file", ""),
                        "timestamp": meta.get("timestamp", ""),
                        "score": doc.get("score", 0.0),
                    }
                )
            )
 
        # Build prompt
        prompt_result = retriever.rag_generator.prompt_builder.run(
            documents=haystack_docs,
            question=query
        )
        prepared_prompts.append(prompt_result["prompt"])
 
    prep_time = time.perf_counter() - prep_start
    print(f"      âœ“ Prompts prepared in {prep_time*1000:.1f}ms (not counted in generation time)")
 
    # Timed generation: Send all requests to vLLM (it handles parallelization)
    generation_start = time.perf_counter()
 
    def send_llm_request(prompt_idx: int) -> Dict:
        """Send a single LLM request (vLLM parallelizes on server side)."""
        prompt = prepared_prompts[prompt_idx]
 
        llm_start = time.perf_counter()
        try:
            generation_result = retriever.rag_generator.generator.run(prompt=prompt)
            if "replies" in generation_result and generation_result["replies"]:
                answer = generation_result["replies"][0]
            else:
                answer = "No answer generated by the model."
            llm_time = time.perf_counter() - llm_start
 
            return {
                "query_idx": prompt_idx,
                "llm_inference_time": llm_time,
                "answer": answer.strip(),
            }
        except Exception as e:
            print(f"      âš ï¸  Query {prompt_idx} failed: {e}")
            return {
                "query_idx": prompt_idx,
                "llm_inference_time": 0.0,
                "answer": f"Error: {e}",
            }
 
    # Send all requests (using threads only for concurrent HTTP requests, vLLM does the real work)
    results = []
    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = [executor.submit(send_llm_request, i) for i in range(len(batch_queries))]
        for future in futures:
            results.append(future.result())
 
    batch_generation_time = time.perf_counter() - generation_start
    overall_time = time.perf_counter() - overall_start
 
    print(f"      âœ“ Generation: {batch_generation_time*1000:.1f}ms")
    print(f"      âœ“ Overall (Retrieval + Generation): {overall_time*1000:.1f}ms")
 
    # Calculate metrics
    amortized_retrieval = batch_retrieval_time / len(batch_queries)
    avg_generation = batch_generation_time / len(batch_queries)
 
    # Overall throughput (queries per second for the entire pipeline)
    throughput = len(batch_queries) / overall_time
 
    # Individual LLM request times (for analysis)
    individual_llm_times = [r["llm_inference_time"] for r in results]
    avg_llm_time = sum(individual_llm_times) / len(individual_llm_times) if individual_llm_times else 0
    max_llm_time = max(individual_llm_times) if individual_llm_times else 0
 
    # Calculate speedup compared to sequential execution
    # Sequential would be: sum of all individual LLM times + batch retrieval time
    sequential_time = sum(individual_llm_times) + batch_retrieval_time
    speedup = sequential_time / overall_time if overall_time > 0 else 1.0
 
    print(f"      ðŸ“Š Throughput: {throughput:.2f} queries/sec")
    print(f"      ðŸ“Š Avg LLM time: {avg_llm_time*1000:.1f}ms, Max: {max_llm_time*1000:.1f}ms")
    print(f"      ðŸ“Š Speedup vs sequential: {speedup:.2f}x")
 
    return {
        "batch_size": batch_size,
        "num_queries": len(batch_queries),
        "rag_workers": rag_workers,
        "batch_retrieval_time_ms": batch_retrieval_time * 1000,
        "batch_generation_time_ms": batch_generation_time * 1000,
        "prompt_prep_time_ms": prep_time * 1000,
        "overall_time_ms": overall_time * 1000,
        "avg_retrieval_time_ms": amortized_retrieval * 1000,
        "avg_generation_time_ms": avg_generation * 1000,
        "avg_llm_inference_time_ms": avg_llm_time * 1000,
        "max_llm_inference_time_ms": max_llm_time * 1000,
        "throughput_queries_per_sec": throughput,
        "speedup": speedup,
    }
 
 
def main():
    parser = argparse.ArgumentParser(
        description="Sequential batch benchmark - loads index once, runs batches sequentially"
    )
 
    # Required arguments
    parser.add_argument("--query-file", type=str, required=True,
                       help="File containing queries (one per line)")
    parser.add_argument("--output-file", type=str, required=True,
                       help="Output file for benchmark results (JSON)")
 
    # RAG system paths
    parser.add_argument("--store-dir", type=str, required=True,
                       help="RAG store directory")
    # LLM settings
    parser.add_argument("--llm-api-url", type=str,
                       default="http://localhost:5000/v1")
    parser.add_argument("--llm-model", type=str,
                       default="openai/gpt-oss-20b")
 
    # Embedding model settings
    parser.add_argument("--model", type=str,
                       default="sentence-transformers/static-retrieval-mrl-en-v1")
    parser.add_argument("--backend", type=str, default="onnx")
    parser.add_argument("--provider", type=str, default="CPUExecutionProvider")
    parser.add_argument("--onnx-file", type=str, default=None)
    parser.add_argument("--truncate-dim", type=int, default=None)
 
    # Performance settings
    parser.add_argument("--omp-threads", type=int, default=64)
    parser.add_argument("--ort-intra", type=int, default=8)
    parser.add_argument("--ort-inter", type=int, default=1)
    parser.add_argument("--embed-batch", type=int, default=128)
    parser.add_argument("--shard-cache", type=int, default=24)
    parser.add_argument("--disable-mmap", action="store_true")
 
    # Retrieval settings
    parser.add_argument("--top-k", type=int, default=5)
    parser.add_argument("--max-chars-per-doc", type=int, default=500)
 
    # Batch sizes to test
    parser.add_argument("--batch-sizes", type=str, default="1,2,4,8,16,32,64,128",
                       help="Comma-separated list of batch sizes to test")
 
    # Max doc workers (for initial loading)
    parser.add_argument("--doc-workers", type=int, default=16,
                       help="Number of doc workers for document fetching")
 
    args = parser.parse_args()
 
    # Parse batch sizes
    batch_sizes = [int(x.strip()) for x in args.batch_sizes.split(",")]
 
    # Load queries
    queries = []
    with open(args.query_file, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                queries.append(line)
 
    if not queries:
        print("Error: No queries found in file", file=sys.stderr)
        return 1
 
    max_batch_size = max(batch_sizes)
    if len(queries) < max_batch_size:
        print(f"Error: Need at least {max_batch_size} queries, but only have {len(queries)}", file=sys.stderr)
        return 1
 
    print("=" * 80)
    print("PARALLEL RETRIEVAL BATCH BENCHMARK")
    print("=" * 80)
    print(f"Strategy: Load index ONCE, run batches SEQUENTIALLY with PARALLEL retrieval")
    print(f"Queries loaded: {len(queries)}")
    print(f"Batch sizes: {batch_sizes}")
    print(f"Thread allocation: batch_size N uses min(N, 16) workers for both retrieval and generation")
    print(f"Worker cap: 16 (optimal for memory bandwidth and thread efficiency)")
    print("=" * 80)
    print()
 
    # Step 1: Initialize retriever ONCE
    print("ðŸ”§ Initializing RAG retriever (happens only ONCE)...")
    init_start = time.perf_counter()
 
    retriever = LargeScaleRAGRetriever(
        store_dir=Path(args.store_dir),
        api_base_url=args.llm_api_url,
        model_name=args.llm_model,
        api_key="EMPTY",
        llm_max_tokens=512,
        llm_temperature=0.1,
        embedding_model=args.model,
        backend=args.backend,
        provider=args.provider,
        onnx_file=args.onnx_file,
        truncate_dim=args.truncate_dim,
        omp_threads=args.omp_threads,
        ort_intra=args.ort_intra,
        ort_inter=args.ort_inter,
        embed_batch=args.embed_batch,
        doc_workers=args.doc_workers,
        shard_cache=args.shard_cache,
        use_mmap=not args.disable_mmap,
    )
 
    init_time = time.perf_counter() - init_start
    print(f"âœ“ Retriever initialized in {init_time:.2f}s")
    print(f"  FAISS index loaded: READY")
    print(f"  Document store loaded: READY")
    print()
 
    # Step 2: Run benchmarks SEQUENTIALLY for each batch size
    benchmark_start = time.perf_counter()
    all_results = []
 
    for batch_size in batch_sizes:
        # Set rag_workers to match batch_size (cap at 16 for optimal performance)
        # Beyond 16 workers, memory bandwidth saturation and thread contention reduce throughput
        rag_workers = min(batch_size, 16)
 
        try:
            result = run_single_batch_benchmark(
                retriever=retriever,
                batch_size=batch_size,
                queries=queries,
                top_k=args.top_k,
                max_chars_per_doc=args.max_chars_per_doc,
                rag_workers=rag_workers,
            )
            all_results.append(result)
 
        except Exception as e:
            print(f"âœ— Batch {batch_size} failed: {e}")
            import traceback
            traceback.print_exc()
 
    total_benchmark_time = time.perf_counter() - benchmark_start
 
    # Cleanup
    retriever.close()
 
    # Summary
    print()
    print("=" * 80)
    print("BENCHMARK SUMMARY")
    print("=" * 80)
    print(f"Initialization Time: {init_time:.2f}s (one-time cost)")
    print(f"Total Benchmark Time: {total_benchmark_time:.2f}s (all batches)")
    print(f"Overall Time: {init_time + total_benchmark_time:.2f}s")
    print()
    print("Results by batch size:")
    print("-" * 80)
    print(f"{'Batch':>6} {'Queries':>8} {'Workers':>8} {'Retrieval(ms)':>14} {'Generation(ms)':>15} {'Total(ms)':>12} {'Throughput':>12} {'Speedup':>10}")
    print("-" * 80)
 
    for result in all_results:
        print(f"{result['batch_size']:>6} {result['num_queries']:>8} "
              f"{result['rag_workers']:>8} "
              f"{result['batch_retrieval_time_ms']:>14.1f} "
              f"{result['batch_generation_time_ms']:>15.1f} "
              f"{result['overall_time_ms']:>12.1f} "
              f"{result['throughput_queries_per_sec']:>12.2f} "
              f"{result['speedup']:>10.2f}x")
 
    print("-" * 80)
 
    # Save results
    output_data = {
        "initialization_time_seconds": init_time,
        "total_benchmark_time_seconds": total_benchmark_time,
        "overall_time_seconds": init_time + total_benchmark_time,
        "batch_sizes_tested": batch_sizes,
        "results": all_results,
    }
 
    with open(args.output_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2)
 
    print()
    print(f"âœ“ Results saved to {args.output_file}")
    print("=" * 80)
 
    return 0
 
 
if __name__ == "__main__":
    sys.exit(main())
 
 